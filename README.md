# ðŸ¤– Persuasion Bot
An LLM-based chatbot that takes in messages from a user, processes them, and generates responses intended to defend itself or maintain its position during an ongoing conversation.

## ðŸ“œ Table of Contents
1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Entities](#entities)
4. [Tech Stack](#tech-stack)
5. [Getting Started](#getting-started)
6. [API Documentation](#api-documentation)
7. [Example Requests](#example-requests)
8. [Non Functional Requirements](#non-functional-requirements)
9. [Database Optimization](#database-optimization)
10. [Production Considerations](#produciton-considerations)
11. [LLM](#llm)

## Overview
This application challenges you to persuade a chatbot to adopt your point of view while it stands its ground on the initial stance.

Includes:
- Single endpoint to handle messages and responses.
- Conversation history capped at the 5 most recent user+bot pairs.

## Architecture
![Architecture Diagram](docs/architecture.png?v=2   )

- **Client**: Anyone consuming the API.
- **API**: REST API built with FastAPI.
- **Database**: ~~In-memory storage for dev/testing~~, PostgreSQL.
- **Cache**: Redis Cache to improve latency and enable rate limiting.

### Caching Strategy

We cache three things to improve latency, cut costs, and avoid duplicate work:

1. Idempotency Keys

    Prevents double processing when clients retry/double-send.

    Key: `idempotency:{request_uuid}` (TTL: 5â€“15 min).

2. Last Messages (Conversation History Cache)

    Stores last N messages of a conversation.

    Key: `conversation:{conversation_id}:history` (TTL: 30â€“60 min, reset on activity).

3. LLM Reply Cache

    Re-uses model reply for identical prompt/context.

    Key: `llm:{sha256(model+system_prompts+topic+side+last_K_msgs+user_msg+temperature)}` (TTL: 1â€“24h).

## Entities
![Entities](docs/entities.png)

Messages will contain ai prompts and responses.

- ```id```: id autogenerated
- ```conversation_id```: related conversation
- ```message```: message content
- ```role```: message owner (user, bot)
- ```timestamps```

Conversations will be used to link every message based on topic

- ```id```: id autogenerated
- ```side```: wheter bot will be against or in favor of (con, pro)
- ```topic```: subject of a conversation or discussion.
- ```timestamps```

## Tech Stack
- **Backend**: FastAPI (Python 3.9+)
- **Database**: ~~In-memory dict for dev/testing~~, PostgreSQL.
- **API Docs**: Swagger UI / ReDoc (auto-generated)
- **Containerization**: Docker

## Getting Started

### Prerequisites (Docker-first)
- Docker (required)
    - Follow Docker installation [instructions](https://docs.docker.com/engine/install/)
- GNU Make (required)
    - On Linux/macOS: preinstalled or install via package manager (sudo apt install make, brew install make)

    - On Windows: install via Chocolatey (choco install make) or MSYS2
- Python & pip (optional, for local-only dev/tests)
- PostgreSQL (optional, for local-only dev)

### Clone Repository
```bash
git clone https://github.com/gonzasestopal/persuasion_bot.git
cd persuasion_bot
```

### Environment
Copy .env.example to .env and adjust if needed:
```
POSTGRES_USER=app
POSTGRES_PASSWORD=app
POSTGRES_DB=app
DATABASE_URL=postgresql://app:app@db:5432/app
OPENAI_API_KEY=key
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o

```

### Running the service
```
make run
```

### Running tests
```
make test
```

### Cleanup
```
make down   # stop Docker services if running (no-op if none exist)
make clean  # remove venv, caches, __pycache__
```

## API Documentation

Base URL:
- **Local:** `http://localhost:8000`

Authentication:
- None is required

### Messages
| Method | Endpoint                 | Description
|--------|--------------------------|-------------------------------------
| POST   | `/messages`              | Inserts a new message / Starts convo

## Example Requests

>  [!IMPORTANT]
>  Make sure the starting message contains topic and side (PRO, CON) in any order.<br>
>  Topic: Dogs are humans best friend Side: pro

**Start a new conversation**
```http
POST /messages HTTP/1.1
Host: http://localhost:8000
Content-Type: application/json
```

```
{
    "conversation_id": null,
    "message": "Topic: Sports are one of the best ways to build discipline and strengthen community bonds. Side: PRO."
}
```
**Response 201**
```
{
    "conversation_id": "1",
    "message": [
        {
            "role": "user",
            "message": "Topic: Sports are one of the best ways to build discipline and strengthen community bonds. Side: PRO."
        },
        {
            "role": "bot",
            "message": "I will gladly take the PRO side. Sports are key in shaping discipline..."
        }
    ]
}
```

**Continue an existing conversation**
```http
POST /messages HTTP/1.1
Host: http://localhost:8000
Content-Type: application/json
```

```
{
    "conversation_id": 1,
    "message": "But sports also divide people"
}
```

**Response 200**
```
{
    "conversation_id": 1,
    "message": [
        {
            "role": "user",
            "message": "Topic: Sports are one of the best ways to build discipline and strengthen community bonds. Side: PRO."
        },
        {
            "role": "bot",
            "message": "I will gladly take the PRO side. Sports are key in shaping discipline..."
        },
        {
            "role": "user",
            "message": "But sports also divide people."
        },
        {
            "role": "bot",
            "message": "I formly believe that sports can unite and divide people in the same amount, only fanatism is the real problem."
        }
    ]
}
```

**Error Examples**
```
{ "detail": "message must not be empty." }                                       # 422 Unprocessable Entity
{ "detail": "message must contain topic and side for starting a conversation." } # 422 Unprocessable Entity
{ "detail": "conversation_id must be null when starting a conversation" }        # 422 Unprocessable Entity
{ "detail": "conversation_id not found or expired" }                             # 404 Not Found
{ "detail": "response generation timed out" }                                    # 503 Bad Gateway
```

## Non Functional Requirements

**Latency**: Response < 30s (hard API cap). Internal timeout enforced at 25s.

**Scalability**: Caching layers reduce duplicate LLM calls and DB load.

**History Window**: Only the last 5 user+bot pairs are returned in responses, taking in consideration the following approaches:

- A) Soft-delete (mark older rows as deleted)

- B) Donâ€™t delete; select â€œlatest Nâ€ with a subquery (and reorder ascending)

For this implementation I went with the subquery approach because itâ€™s simpler and keeps the write path lightweight. A soft-delete strategy would allow undelete/auditability, but it also incurs extra overhead by updating indexes

**Fault Tolerance**: If timeout triggers, bot responds with a short fallback argument.

**Storage**: Conversations expire after 60 minutes of inactivity.

## Database Optimization

- **conversations (expires_at)**
  - Speeds up lookups for active conversations (`expires_at > NOW()`)
  - Used in cleanup job to quickly delete expired rows

- **messages (conversation_id, created_at)**
  - Optimizes retrieval of the last N messages for a conversation
  - Maintains ordering by creation time for fast sequential reads

- **messages (conversation_id, created_at DESC, id DESC)**
  - Optimizes â€œlatest Nâ€ queries like `WHERE conversation_id = $1 ORDER BY created_at DESC, id DESC LIMIT $2` by satisfying both sort keys from the index (often avoiding an extra sort).
  - Uses the monotonic primary key `id` as a deterministic tie-breaker when multiple rows share the same `created_at`, preserving userâ†’bot ordering as required.

## Production Considerations

For simplicity, expired conversations will not be physically deleted from the database in this implementation. In a production system, you would typically run a periodic cleanup job to purge expired rows.

## LLM
We consider both **Anthropic** and **OpenAI** providers. We prioritize **fast response** for our low-latency cap; thereâ€™s no hard token or cost constraint in scope.

We target a **medium-resistance debate bot** and budget prompt length for latency predictability.

> [!IMPORTANT]
> **Prompt budget:** keep input â‰¤ **2.5â€“3k tokens**.
> **Structure**: system prompt + last **every user bot messages**.
> *(We will consider a summary for longer conversations)*
> **Output cap:** aim 80â€“120 tokens (hard cap 400).

### GPT-4o
Optimized for speed: first tokens in ~1â€“2s, full ~300â€“500 token reply often well under 10s.
Even long (1kâ€“1.5k token) answers usually <20s.

### Claude 3.5
**Sonnet**: typically <10s for medium answers; competitive with GPT-4o.
**Opus**: slower â€” long answers can push past 30s if you donâ€™t stream or limit length.

### Capacity Check (token math, approximate)

**Building blocks**
- **System (S):** ~120â€“180 tokens
- **One pair (P = user+bot, with wrapper):** ~52â€“73 tokens

**Totals**
`Total(N pairs) = S + NÂ·P`

- **1 pair:** `120â€“180 + 52â€“73 = ~172â€“253`
- **5 pairs (runtime soft cap):** `= ~380â€“545`
- **37 pairs (worst-case safe under 3k):** `= ~2,044â€“2,881`
- **42 pairs (Mid-target near 3k):**
 gives ~**2,750** mid-case
  *(Note: worst-case at N=45 can exceed 3k; we will raise error for now.)*

### Conclusion
Given our strict **<30s** response requirement, **GPT-4o** is the safer default: fast first tokens and consistent completion under 10s for medium replies.
**Claude 3.5 Sonnet** is a solid alternative when a more conversational style is desired; keep output caps tighter to protect latency.
**Claude 3.5 Opus** is excluded due to slower generation, risking SLA breaches without streaming/caps.

> Note: OpenAI and Anthropic have different tokenizers; counts vary slightly. We use the same budgeting pattern and enforce a â‰¤3k input guard at send time.

> Note: For conversations that push the input context beyond 3k tokens, we will need to introduce a running summary or trimming strategy to keep requests within budget.
